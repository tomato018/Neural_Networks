---
title: "Neural Networks"
author: "Meijuan Zeng"
date: "3/26/2019"
output: html_document
---

## Part I - Introduction to Using Neural Nets

In the attached data sets attention1.csv and attention2.csv, you will find data that describe features assocaited with webcam images of 100 students' faces as they particpate in an online discussion. The variables are:

eyes - student has their eyes open (1 = yes, 0 = no)
face.forward - student is facing the camera (1 = yes, 0 = no)
chin.up - student's chin is raised above 45 degrees (1 = yes, 0 = no)
attention - whether the student was paying attention when asked (1 = yes, 0 = no)

We will use the webcam data to build a neural net to predict whether or not a student is attending.

First install and load the neuralnet package
```{r}
install.packages("neuralnet")
library(neuralnet)
library(dplyr)
```

Now upload your data
```{r}
D1 <-read.csv('attention1.csv',header = TRUE)
  
D2 <-read.csv('attention2.csv',header = TRUE)
```

Now you can build a neural net that predicts attention based on webcam images. The command "neuralnet" sets up the model. It is composed of four basic arguments:

- A formula that describes the inputs and outputs of the neural net (attention is our output)
- The data frame that the model will use
- How many hidden layers are in our neural net
- A threshold that tells the model when to stop adjusting weights to find a better fit. If error does not change more than the threshold from one iteration to the next, the algorithm will stop (We will use 0.01, so if prediction error does not change by more than 1% from one iteration to the next the algorithm will halt)

```{r}
net <- neuralnet(attention ~ eyes + face.forward + chin.up, D1, hidden = 1, threshold = 0.01)

plot(net)
```

You have now trained a neural network! The plot shows you the layers of your newtork as black nodes and edges with the calculated weights on each edge. The blue nodes and edges are called bias terms. The bias term anchors the activation function, the weights change the shape of the activation function while the bias term changes the overall position of the activation function - if you have used linear regressionthe bias term is like the intercept of the regression equation, it shifts the trend line up and down the y axis, while the other parameters change the angle of the line. The plot also reports the final error rate and the number of iterations ("steps") that it took to reach these weights.

What happens if you increase the number of hidden layers in the neural net? Build a second neural net with more layers in it and determine if this iproves your predictions or not? How can you tell if your new neural network is doing a better job than your first?

```{r}
net1 <- neuralnet(attention ~ eyes + face.forward + chin.up, D1, hidden = 5, threshold = 0.01)

plot(net1)

#If I increase the number of hidden layers to, for example, 5, both the number of iterations (steps) that it took to reach these weights and the final error rate to predict one's attention level decrease.From the fewer steps and lower error rate in this nerual network, I can tell that this one is doing a better job than the first. 
```

Now use your preferred neural net to predict the second data set. You will need to create a new data frame (D3) that only includes the input layers to use this command.

```{r}
D3 <- D2[,1:3]
```

Now you can create predictions using your neural net
```{r}
net.prediction <- neuralnet::compute(net, D3)
D4 <- data.frame(net.prediction$net.result)
prediction <- ifelse(D4$net.prediction.net.result <0.5, 0, 1)
table(prediction, D2$attention)

#You can access the predictions from your model as "net.prediction$net.result". Predictions will be numeric estimates from 1 or 0, convert these into exact predictions of 1 and 0 and then determine the accuracy of your neural net on this new data.

#The accuracy is (41+53)/(41+53+3+3) = 0.94

net.prediction <- neuralnet::compute(net1, D3)
D5 <- data.frame(net.prediction$net.result)
prediction1 <- ifelse(D5$net.prediction.net.result <0.5, 0, 1)
table(prediction1, D2$attention)
#The accuracy is (41+53)/(41+53+3+3) = 0.94

```

## Please answer the following questions:

1. How accurate is your neural net? How can you tell?
#Both of my neural nets have the same accuracy 94%, despite that the second one has fewer numbers of iteration and lower error rate. I calculate the accuracy by dividing correct prediciton number by the total prediction number.

2. How would you explain your model to the students whose behavior you are predicting? 
#I can explain that students' attention is predicted based on 3 inputs: whether they open eyes, have face facing the camera, and have chins up raising above 45 degrees.

3. This is a very simple example of a neural network. Real facial recognition is very complex though. Would a neural network be a good solution for predicting real facial movements? Why, why not? 
#I think neural networks will be a good solution for predicting the real facial movements, because it can recognize different edges and patterns in different layers and it can tell which edges may be more important factors in predicting students' attention. This is very useful to help teachers identify which students do not pay attention to the class and give them reminders. However, it is harder to say accurately whether a student pays attention based on his or her facial movement. If we train the model constantly, the final results will be better. 
